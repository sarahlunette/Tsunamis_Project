[2024-12-20T00:00:13.283+0000] {processor.py:161} INFO - Started process (PID=2141) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:00:13.284+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:00:13.287+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:13.287+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:00:13.484+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:13.483+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:00:13.801+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:00:13.802+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:13.801+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:00:14.175+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:00:14.176+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:14.176+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:00:14.210+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:00:14.211+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:14.210+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:00:14.629+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:00:14.629+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:14.629+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:00:16.794+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:00:16.818+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:16.818+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:00:16.841+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:16.841+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:00:16.879+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.601 seconds
[2024-12-20T00:00:46.994+0000] {processor.py:161} INFO - Started process (PID=2229) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:00:46.995+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:00:46.997+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:46.997+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:00:47.189+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:47.189+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:00:47.391+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:00:47.392+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:47.392+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:00:47.764+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:00:47.765+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:47.765+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:00:47.812+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:00:47.813+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:47.813+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:00:48.192+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:00:48.193+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:48.193+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:00:50.482+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:00:50.507+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:50.507+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:00:50.528+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:00:50.528+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:00:50.560+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.571 seconds
[2024-12-20T00:01:21.170+0000] {processor.py:161} INFO - Started process (PID=2322) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:01:21.171+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:01:21.175+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:21.174+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:01:21.387+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:21.386+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:01:21.582+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:01:21.583+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:21.583+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:01:21.960+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:01:21.961+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:21.961+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:01:22.016+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:01:22.017+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:22.017+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:01:22.436+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:01:22.437+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:22.437+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:01:24.814+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:01:24.846+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:24.845+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:01:24.881+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:24.881+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:01:24.921+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.755 seconds
[2024-12-20T00:01:55.777+0000] {processor.py:161} INFO - Started process (PID=2394) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:01:55.778+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:01:55.781+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:55.781+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:01:55.986+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:55.985+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:01:56.203+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:01:56.204+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:56.203+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:01:56.670+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:01:56.672+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:56.671+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:01:56.740+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:01:56.742+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:56.741+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:01:57.118+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:01:57.119+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:57.119+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:01:59.464+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:01:59.487+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:59.486+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:01:59.510+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:01:59.509+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:01:59.539+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.767 seconds
[2024-12-20T00:02:30.531+0000] {processor.py:161} INFO - Started process (PID=2473) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:02:30.533+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:02:30.538+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:02:30.537+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:02:30.758+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:02:30.758+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:02:30.949+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:02:30.950+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:02:30.949+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:02:31.299+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:02:31.300+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:02:31.300+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:02:31.354+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:02:31.355+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:02:31.355+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:02:31.715+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:02:31.716+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:02:31.716+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:02:33.775+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:02:33.798+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:02:33.797+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:02:33.967+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:02:33.967+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:02:33.989+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.469 seconds
[2024-12-20T00:03:04.343+0000] {processor.py:161} INFO - Started process (PID=2554) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:03:04.344+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:03:04.347+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:04.346+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:03:04.533+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:04.533+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:03:04.727+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:03:04.728+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:04.727+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:03:05.062+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:03:05.063+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:05.063+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:03:05.102+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:03:05.102+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:05.102+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:03:05.437+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:03:05.438+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:05.437+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:03:07.604+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:03:07.629+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:07.628+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:03:07.652+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:07.652+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:03:07.837+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.498 seconds
[2024-12-20T00:03:37.925+0000] {processor.py:161} INFO - Started process (PID=2626) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:03:37.926+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:03:37.929+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:37.928+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:03:38.148+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:38.148+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:03:38.345+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:03:38.346+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:38.345+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:03:38.690+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:03:38.691+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:38.691+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:03:38.727+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:03:38.728+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:38.728+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:03:39.122+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:03:39.123+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:39.123+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:03:41.149+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:03:41.171+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:41.171+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:03:41.197+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:03:41.197+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:03:41.219+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.298 seconds
[2024-12-20T00:04:11.560+0000] {processor.py:161} INFO - Started process (PID=2705) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:04:11.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:04:11.564+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:11.564+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:04:11.766+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:11.765+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:04:12.020+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:04:12.021+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:12.021+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:04:12.407+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:04:12.408+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:12.408+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:04:12.477+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:04:12.478+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:12.478+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:04:12.864+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:04:12.865+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:12.865+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:04:15.028+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:04:15.055+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:15.054+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:04:15.077+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:15.077+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:04:15.101+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.545 seconds
[2024-12-20T00:04:45.374+0000] {processor.py:161} INFO - Started process (PID=2789) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:04:45.376+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:04:45.380+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:45.380+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:04:45.594+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:45.594+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:04:45.802+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:04:45.803+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:45.803+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:04:46.149+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:04:46.150+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:46.150+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:04:46.212+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:04:46.213+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:46.213+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:04:46.675+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:04:46.676+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:46.676+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:04:48.936+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:04:48.956+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:48.956+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:04:48.980+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:04:48.980+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:04:49.003+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.637 seconds
[2024-12-20T00:05:19.134+0000] {processor.py:161} INFO - Started process (PID=2881) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:05:19.135+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:05:19.138+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:19.137+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:05:19.353+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:19.352+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:05:19.611+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:05:19.611+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:19.611+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:05:19.954+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:05:19.955+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:19.955+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:05:19.990+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:05:19.990+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:19.990+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:05:20.333+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:05:20.334+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:20.333+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:05:22.734+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:05:22.770+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:22.769+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:05:22.811+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:22.811+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:05:22.855+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.725 seconds
[2024-12-20T00:05:53.557+0000] {processor.py:161} INFO - Started process (PID=2963) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:05:53.560+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:05:53.564+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:53.563+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:05:53.779+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:53.779+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:05:53.971+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:05:53.972+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:53.972+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:05:54.300+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:05:54.301+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:54.301+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:05:54.345+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:05:54.346+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:54.345+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:05:54.684+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:05:54.686+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:54.685+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:05:57.079+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:05:57.107+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:57.106+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:05:57.290+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:05:57.290+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:05:57.324+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.774 seconds
[2024-12-20T00:06:27.692+0000] {processor.py:161} INFO - Started process (PID=3054) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:06:27.693+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:06:27.696+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:06:27.695+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:06:27.945+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:06:27.945+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:06:28.273+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:06:28.274+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:06:28.274+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:06:28.661+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:06:28.662+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:06:28.662+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:06:28.721+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:06:28.722+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:06:28.722+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:06:29.077+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:06:29.078+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:06:29.077+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:06:31.079+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:06:31.108+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:06:31.107+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:06:31.303+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:06:31.303+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:06:31.338+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.651 seconds
[2024-12-20T00:07:02.056+0000] {processor.py:161} INFO - Started process (PID=3133) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:07:02.058+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:07:02.061+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:02.060+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:07:02.251+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:02.251+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:07:02.436+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:07:02.437+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:02.436+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:07:02.806+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:07:02.807+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:02.806+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:07:02.846+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:07:02.847+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:02.847+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:07:03.197+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:07:03.198+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:03.198+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:07:05.423+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:07:05.448+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:05.447+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:07:05.470+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:05.469+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:07:05.502+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.454 seconds
[2024-12-20T00:07:35.956+0000] {processor.py:161} INFO - Started process (PID=3205) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:07:35.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:07:35.961+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:35.961+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:07:36.156+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:36.156+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:07:36.375+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:07:36.377+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:36.376+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:07:36.792+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:07:36.793+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:36.793+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:07:36.829+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:07:36.830+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:36.829+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:07:37.154+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:07:37.155+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:37.155+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:07:39.265+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:07:39.297+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:39.297+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:07:39.322+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:07:39.321+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:07:39.353+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.400 seconds
[2024-12-20T00:08:09.665+0000] {processor.py:161} INFO - Started process (PID=3284) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:08:09.666+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:08:09.670+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:09.669+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:08:09.858+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:09.857+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:08:10.054+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:08:10.056+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:10.056+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:08:10.425+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:08:10.426+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:10.426+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:08:10.484+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:08:10.485+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:10.485+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:08:10.842+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:08:10.843+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:10.843+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:08:12.962+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:08:12.984+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:12.984+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:08:13.006+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:13.005+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:08:13.035+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.375 seconds
[2024-12-20T00:08:43.669+0000] {processor.py:161} INFO - Started process (PID=3357) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:08:43.671+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:08:43.674+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:43.673+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:08:43.851+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:43.851+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:08:44.150+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:08:44.151+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:44.151+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:08:44.492+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:08:44.492+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:44.492+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:08:44.530+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:08:44.530+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:44.530+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:08:44.861+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:08:44.862+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:44.862+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:08:47.346+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:08:47.377+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:47.376+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:08:47.402+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:08:47.402+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:08:47.420+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.756 seconds
[2024-12-20T00:09:18.017+0000] {processor.py:161} INFO - Started process (PID=3442) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:09:18.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:09:18.023+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:18.023+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:09:18.232+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:18.231+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:09:18.553+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:09:18.553+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:18.553+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:09:18.898+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:09:18.899+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:18.899+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:09:18.953+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:09:18.954+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:18.954+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:09:19.340+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:09:19.341+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:19.341+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:09:21.529+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:09:21.569+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:21.568+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:09:21.593+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:21.593+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:09:21.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.618 seconds
[2024-12-20T00:09:52.497+0000] {processor.py:161} INFO - Started process (PID=3533) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:09:52.499+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:09:52.502+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:52.501+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:09:52.742+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:52.741+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:09:53.078+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:09:53.079+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:53.079+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:09:53.425+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:09:53.427+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:53.426+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:09:53.492+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:09:53.494+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:53.493+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:09:53.861+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:09:53.862+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:53.862+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:09:56.001+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:09:56.021+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:56.020+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:09:56.042+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:09:56.042+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:09:56.064+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.571 seconds
[2024-12-20T00:10:26.633+0000] {processor.py:161} INFO - Started process (PID=3614) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:10:26.634+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:10:26.638+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:10:26.637+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:10:26.913+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:10:26.912+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:10:27.229+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:10:27.230+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:10:27.230+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:10:27.589+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:10:27.589+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:10:27.589+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:10:27.621+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:10:27.622+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:10:27.622+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:10:27.959+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:10:27.959+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:10:27.959+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:10:30.358+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:10:30.385+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:10:30.385+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:10:30.424+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:10:30.423+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:10:30.450+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.822 seconds
[2024-12-20T00:11:00.944+0000] {processor.py:161} INFO - Started process (PID=3705) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:11:00.946+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:11:00.950+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:00.950+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:11:01.178+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:01.178+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:11:01.363+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:11:01.364+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:01.364+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:11:01.702+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:11:01.703+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:01.702+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:11:01.774+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:11:01.775+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:01.775+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:11:02.190+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:11:02.191+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:02.191+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:11:04.394+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:11:04.413+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:04.413+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:11:04.432+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:04.432+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:11:04.458+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.524 seconds
[2024-12-20T00:11:35.348+0000] {processor.py:161} INFO - Started process (PID=3787) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:11:35.350+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:11:35.352+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:35.352+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:11:35.557+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:35.557+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:11:35.744+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:11:35.745+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:35.745+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:11:36.152+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:11:36.153+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:36.153+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:11:36.189+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:11:36.190+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:36.190+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:11:36.570+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:11:36.571+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:36.571+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:11:39.009+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:11:39.033+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:39.032+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:11:39.054+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:11:39.053+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:11:39.089+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.745 seconds
[2024-12-20T00:12:09.676+0000] {processor.py:161} INFO - Started process (PID=3866) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:12:09.678+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:12:09.681+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:12:09.680+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:12:09.881+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:12:09.881+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:12:10.313+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:12:10.315+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:12:10.314+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:12:10.665+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:12:10.666+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:12:10.666+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:12:10.713+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:12:10.713+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:12:10.713+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:12:11.063+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:12:11.064+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:12:11.064+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:12:13.235+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:12:13.260+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:12:13.259+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T00:12:13.281+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:12:13.281+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T00:12:13.316+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.644 seconds
[2024-12-20T00:12:44.223+0000] {processor.py:161} INFO - Started process (PID=3945) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:12:44.287+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T00:12:44.349+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:12:44.347+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T00:12:45.848+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:12:45.847+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T00:12:46.341+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T00:12:46.370+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:12:46.369+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T00:12:47.570+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:12:47.570+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:12:47.570+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T00:12:47.605+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T00:12:47.606+0000] {logging_mixin.py:188} INFO - [2024-12-20T00:12:47.606+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T03:12:50.485+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T03:12:50.499+0000] {logging_mixin.py:188} INFO - [2024-12-20T03:12:50.488+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:17:31.937+0000] {processor.py:161} INFO - Started process (PID=4017) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:17:31.943+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:17:31.949+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:17:31.947+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:17:33.391+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:17:33.390+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:17:34.222+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:17:34.226+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:17:34.224+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:17:35.041+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:17:35.042+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:17:35.042+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:17:35.073+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:17:35.075+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:17:35.075+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:17:36.396+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:17:36.397+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:17:36.397+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:17:43.908+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:17:44.028+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:17:44.028+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:17:44.051+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:17:44.050+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T09:17:44.088+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 13.363 seconds
[2024-12-20T09:18:14.734+0000] {processor.py:161} INFO - Started process (PID=4097) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:18:14.736+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:18:14.737+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:14.737+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:18:14.932+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:14.932+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:18:15.178+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:18:15.179+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:15.179+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:18:15.555+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:18:15.555+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:15.555+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:18:15.603+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:18:15.604+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:15.604+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:18:16.026+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:18:16.027+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:16.027+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:18:18.461+0000] {processor.py:840} INFO - DAG(s) 'my_dag_2' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:18:18.483+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:18.483+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:18:18.505+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:18.505+0000] {dag.py:3823} INFO - Setting next_dagrun for my_dag_2 to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T09:18:18.536+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.806 seconds
[2024-12-20T09:18:43.359+0000] {processor.py:161} INFO - Started process (PID=4174) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:18:43.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:18:43.362+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:43.361+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:18:43.635+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:43.634+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:18:43.871+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:18:43.872+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:43.872+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:18:44.328+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:18:44.328+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:44.328+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:18:44.364+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:18:44.365+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:44.365+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:18:44.766+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:18:44.767+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:44.767+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:18:48.072+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:18:48.213+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:48.212+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:training_pipeline
[2024-12-20T09:18:48.233+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:48.232+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:training_pipeline
[2024-12-20T09:18:48.246+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:48.246+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:training_pipeline
[2024-12-20T09:18:48.248+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:48.247+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:18:48.269+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:48.268+0000] {dag.py:3058} INFO - Creating ORM DAG for training_pipeline
[2024-12-20T09:18:48.288+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:18:48.288+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T09:18:48.327+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.973 seconds
[2024-12-20T09:19:05.676+0000] {processor.py:161} INFO - Started process (PID=4229) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:19:05.680+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:19:05.683+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:05.682+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:19:05.996+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:05.996+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:19:16.797+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:19:16.798+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:16.798+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:19:17.796+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:19:17.798+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:17.797+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:19:17.840+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:19:17.840+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:17.840+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:19:19.581+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:19:19.582+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:19.582+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:19:27.411+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:19:27.467+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:27.466+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:19:27.515+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:27.515+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T09:19:27.553+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 21.886 seconds
[2024-12-20T09:19:57.856+0000] {processor.py:161} INFO - Started process (PID=4343) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:19:57.858+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:19:57.860+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:57.860+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:19:58.196+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:58.196+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:19:58.497+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:19:58.499+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:58.498+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:19:58.988+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:19:58.988+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:58.988+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:19:59.022+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:19:59.023+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:59.023+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:19:59.502+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:19:59.503+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:19:59.503+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:20:03.022+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:20:03.050+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:20:03.049+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:20:03.090+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:20:03.090+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T09:20:03.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 5.285 seconds
[2024-12-20T09:20:33.803+0000] {processor.py:161} INFO - Started process (PID=4423) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:20:33.807+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:20:33.810+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:20:33.809+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:20:34.424+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:20:34.424+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:20:39.504+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:20:39.504+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:20:39.504+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:20:40.044+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:20:40.045+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:20:40.045+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:20:40.088+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:20:40.089+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:20:40.089+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:20:40.545+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:20:40.546+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:20:40.546+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:20:44.051+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:20:44.079+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:20:44.079+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:20:44.118+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:20:44.118+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-10-27 00:00:00+00:00, run_after=2024-10-28 00:00:00+00:00
[2024-12-20T09:20:44.157+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 10.368 seconds
[2024-12-20T09:21:14.314+0000] {processor.py:161} INFO - Started process (PID=4511) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:21:14.317+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:21:14.320+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:21:14.319+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:21:14.607+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:21:14.606+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:21:14.855+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:21:14.856+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:21:14.856+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:21:15.439+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:21:15.440+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:21:15.440+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:21:15.491+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:21:15.492+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:21:15.492+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:21:16.406+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:21:16.408+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:21:16.408+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:21:23.692+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:21:23.862+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:21:23.862+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:21:24.103+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:21:24.102+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-11-09 00:00:00+00:00, run_after=2024-11-10 00:00:00+00:00
[2024-12-20T09:21:24.239+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 9.933 seconds
[2024-12-20T09:21:55.025+0000] {processor.py:161} INFO - Started process (PID=4601) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:21:55.045+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:21:55.049+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:21:55.048+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:21:56.215+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:21:56.214+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:22:04.171+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:22:04.172+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:04.172+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:22:04.883+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:22:04.890+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:04.889+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:22:04.955+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:22:04.957+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:04.956+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:22:06.084+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:22:06.085+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:06.085+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:22:12.213+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:22:12.256+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:12.255+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:22:12.307+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:12.306+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-11-27 00:00:00+00:00, run_after=2024-11-28 00:00:00+00:00
[2024-12-20T09:22:12.397+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 17.393 seconds
[2024-12-20T09:22:42.829+0000] {processor.py:161} INFO - Started process (PID=4708) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:22:42.833+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:22:42.835+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:42.834+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:22:43.237+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:43.237+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:22:43.530+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:22:43.533+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:43.532+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:22:44.469+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:22:44.477+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:44.477+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:22:44.552+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:22:44.553+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:44.553+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:22:45.349+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:22:45.361+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:45.361+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:22:51.628+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:22:51.801+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:51.800+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:22:51.899+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:22:51.898+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-14 00:00:00+00:00, run_after=2024-12-15 00:00:00+00:00
[2024-12-20T09:22:52.016+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 9.194 seconds
[2024-12-20T09:23:22.645+0000] {processor.py:161} INFO - Started process (PID=4794) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:23:22.647+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:23:22.649+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:22.649+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:23:22.930+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:22.930+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:23:23.174+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:23:23.175+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:23.175+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:23:23.711+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:23:23.712+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:23.711+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:23:23.747+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:23:23.748+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:23.747+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:23:24.195+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:23:24.196+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:24.196+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:23:27.387+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:23:27.413+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:27.412+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:23:27.441+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:27.441+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:23:27.476+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.839 seconds
[2024-12-20T09:23:57.664+0000] {processor.py:161} INFO - Started process (PID=4873) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:23:57.667+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:23:57.670+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:57.669+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:23:58.048+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:58.048+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:23:58.263+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:23:58.264+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:58.264+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:23:58.755+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:23:58.757+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:58.756+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:23:58.826+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:23:58.827+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:58.826+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:23:59.315+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:23:59.316+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:23:59.316+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:24:02.463+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:24:02.501+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:24:02.501+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:24:02.543+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:24:02.542+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:24:02.579+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.927 seconds
[2024-12-20T09:24:32.827+0000] {processor.py:161} INFO - Started process (PID=4957) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:24:32.829+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:24:32.833+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:24:32.831+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:24:33.155+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:24:33.155+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:24:41.815+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:24:41.816+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:24:41.816+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:24:42.407+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:24:42.408+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:24:42.408+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:24:42.469+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:24:42.470+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:24:42.470+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:24:43.042+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:24:43.043+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:24:43.043+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:24:46.041+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:24:46.069+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:24:46.069+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:24:46.108+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:24:46.108+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:24:46.145+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 13.327 seconds
[2024-12-20T09:25:16.897+0000] {processor.py:161} INFO - Started process (PID=5066) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:25:16.899+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:25:16.900+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:16.900+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:25:17.160+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:17.160+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:25:17.383+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:25:17.384+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:17.384+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:25:17.894+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:25:17.895+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:17.894+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:25:17.936+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:25:17.937+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:17.937+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:25:18.440+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:25:18.441+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:18.440+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:25:21.635+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:25:21.667+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:21.666+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:25:21.707+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:21.706+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:25:21.751+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.860 seconds
[2024-12-20T09:25:52.444+0000] {processor.py:161} INFO - Started process (PID=5145) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:25:52.445+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:25:52.447+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:52.447+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:25:52.770+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:52.770+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:25:53.092+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:25:53.094+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:53.093+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:25:53.574+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:25:53.575+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:53.575+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:25:53.614+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:25:53.615+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:53.615+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:25:54.233+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:25:54.234+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:54.234+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:25:57.177+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:25:57.214+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:57.213+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:25:57.256+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:25:57.255+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:25:57.301+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.865 seconds
[2024-12-20T09:26:28.251+0000] {processor.py:161} INFO - Started process (PID=5219) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:26:28.252+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:26:28.254+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:26:28.254+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:26:28.491+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:26:28.491+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:26:28.701+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:26:28.702+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:26:28.702+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:26:29.110+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:26:29.111+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:26:29.111+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:26:29.156+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:26:29.157+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:26:29.157+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:26:29.656+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:26:29.657+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:26:29.657+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:26:32.451+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:26:32.476+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:26:32.475+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:26:32.504+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:26:32.503+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:26:32.528+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.283 seconds
[2024-12-20T09:27:02.969+0000] {processor.py:161} INFO - Started process (PID=5307) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:27:02.971+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:27:02.972+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:02.972+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:27:03.224+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:03.224+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:27:03.451+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:27:03.452+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:03.452+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:27:03.897+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:27:03.898+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:03.898+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:27:03.971+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:27:03.973+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:03.972+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:27:04.492+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:27:04.493+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:04.493+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:27:07.235+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:27:07.262+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:07.261+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:27:07.295+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:07.294+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:27:07.337+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.374 seconds
[2024-12-20T09:27:37.506+0000] {processor.py:161} INFO - Started process (PID=5389) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:27:37.508+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:27:37.510+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:37.509+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:27:37.732+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:37.732+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:27:38.648+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:27:38.650+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:38.649+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:27:39.073+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:27:39.074+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:39.073+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:27:39.116+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:27:39.117+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:39.117+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:27:39.597+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:27:39.598+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:39.598+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:27:42.477+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:27:42.502+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:42.502+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:27:42.535+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:27:42.534+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:27:42.584+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 5.083 seconds
[2024-12-20T09:28:12.909+0000] {processor.py:161} INFO - Started process (PID=5476) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:28:12.914+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:28:12.918+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:12.917+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:28:13.199+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:13.198+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:28:13.634+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:28:13.636+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:13.635+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:28:13.906+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:28:13.907+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:13.906+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:28:13.969+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:28:13.971+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:13.970+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:28:14.818+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:28:14.820+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:14.819+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:28:17.905+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:28:17.940+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:17.939+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:28:18.005+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:18.005+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:28:18.044+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 5.145 seconds
[2024-12-20T09:28:48.842+0000] {processor.py:161} INFO - Started process (PID=5555) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:28:48.844+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:28:48.846+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:48.845+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:28:49.129+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:49.129+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:28:49.547+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:28:49.548+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:49.548+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:28:49.980+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:28:49.981+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:49.980+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:28:50.017+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:28:50.018+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:50.017+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:28:50.547+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:28:50.548+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:50.548+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:28:53.322+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:28:53.347+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:53.346+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:28:53.373+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:28:53.373+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:28:53.405+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.571 seconds
[2024-12-20T09:29:23.504+0000] {processor.py:161} INFO - Started process (PID=5646) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:29:23.505+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:29:23.506+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:29:23.506+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:29:23.725+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:29:23.724+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:29:24.072+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:29:24.074+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:29:24.073+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:29:24.533+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:29:24.534+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:29:24.534+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:29:24.578+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:29:24.579+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:29:24.579+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:29:24.994+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:29:24.995+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:29:24.995+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:29:27.673+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:29:27.699+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:29:27.698+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:29:27.726+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:29:27.726+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:29:27.758+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.258 seconds
[2024-12-20T09:29:58.291+0000] {processor.py:161} INFO - Started process (PID=5719) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:29:58.293+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:29:58.296+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:29:58.295+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:29:58.812+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:29:58.812+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:29:59.232+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:29:59.234+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:29:59.233+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:29:59.529+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:29:59.530+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:29:59.530+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:29:59.599+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:29:59.601+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:29:59.600+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:30:00.255+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:30:00.257+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:30:00.256+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:30:03.081+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:30:03.107+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:30:03.106+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:30:03.132+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:30:03.132+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:30:03.165+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.884 seconds
[2024-12-20T09:30:33.387+0000] {processor.py:161} INFO - Started process (PID=5801) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:30:33.389+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:30:33.390+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:30:33.390+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:30:33.610+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:30:33.610+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:30:34.049+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:30:34.050+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:30:34.050+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:30:34.297+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:30:34.298+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:30:34.298+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:30:34.333+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:30:34.334+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:30:34.334+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:30:34.954+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:30:34.955+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:30:34.955+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:30:37.514+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:30:37.543+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:30:37.542+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:30:37.569+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:30:37.568+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:30:37.594+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.212 seconds
[2024-12-20T09:31:07.722+0000] {processor.py:161} INFO - Started process (PID=5881) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:31:07.724+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:31:07.727+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:07.726+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:31:07.994+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:07.993+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:31:08.261+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:31:08.262+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:08.262+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:31:08.842+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:31:08.843+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:08.842+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:31:08.898+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:31:08.899+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:08.899+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:31:09.388+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:31:09.389+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:09.389+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:31:12.328+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:31:12.352+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:12.351+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:31:12.382+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:12.382+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:31:12.411+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.695 seconds
[2024-12-20T09:31:43.042+0000] {processor.py:161} INFO - Started process (PID=5969) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:31:43.044+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:31:43.046+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:43.045+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:31:43.459+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:43.459+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:31:43.786+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:31:43.787+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:43.787+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:31:44.036+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:31:44.037+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:44.037+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:31:44.074+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:31:44.075+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:44.075+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:31:44.668+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:31:44.669+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:44.669+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:31:47.820+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:31:47.855+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:47.854+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:31:47.895+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:31:47.895+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:31:47.940+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.904 seconds
[2024-12-20T09:32:18.052+0000] {processor.py:161} INFO - Started process (PID=6051) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:32:18.055+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:32:18.058+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:18.057+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:32:18.624+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:18.623+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:32:18.858+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:32:18.859+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:18.858+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:32:19.145+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:32:19.146+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:19.146+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:32:19.180+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:32:19.180+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:19.180+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:32:19.917+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:32:19.919+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:19.918+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:32:22.949+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:32:22.972+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:22.971+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:32:23.001+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:23.001+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:32:23.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.983 seconds
[2024-12-20T09:32:53.172+0000] {processor.py:161} INFO - Started process (PID=6138) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:32:53.173+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:32:53.175+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:53.175+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:32:53.641+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:53.641+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:32:53.858+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:32:53.858+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:53.858+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:32:54.115+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:32:54.116+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:54.116+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:32:54.156+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:32:54.157+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:54.157+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:32:54.792+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:32:54.793+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:54.793+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:32:57.659+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:32:57.696+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:57.696+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:32:57.736+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:32:57.736+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:32:57.782+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.616 seconds
[2024-12-20T09:33:28.521+0000] {processor.py:161} INFO - Started process (PID=6221) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:33:28.523+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:33:28.525+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:33:28.525+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:33:29.009+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:33:29.008+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:33:29.346+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:33:29.347+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:33:29.347+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:33:29.634+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:33:29.635+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:33:29.635+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:33:29.705+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:33:29.706+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:33:29.706+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:33:30.316+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:33:30.318+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:33:30.317+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:33:32.994+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:33:33.019+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:33:33.019+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:33:33.045+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:33:33.044+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:33:33.078+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.566 seconds
[2024-12-20T09:34:03.481+0000] {processor.py:161} INFO - Started process (PID=6300) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:34:03.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:34:03.483+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:03.483+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:34:03.711+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:03.711+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:34:04.178+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:34:04.180+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:04.179+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:34:04.516+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:34:04.517+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:04.517+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:34:04.552+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:34:04.553+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:04.553+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:34:05.200+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:34:05.202+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:05.201+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:34:08.257+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:34:08.288+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:08.287+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:34:08.323+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:08.323+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:34:08.352+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.877 seconds
[2024-12-20T09:34:38.677+0000] {processor.py:161} INFO - Started process (PID=6378) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:34:38.681+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:34:38.684+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:38.683+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:34:39.238+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:39.237+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:34:39.632+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:34:39.634+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:39.633+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:34:39.901+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:34:39.902+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:39.902+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:34:39.971+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:34:39.972+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:39.972+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:34:40.733+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:34:40.734+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:40.734+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:34:43.803+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:34:43.828+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:43.827+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:34:43.860+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:34:43.860+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:34:43.909+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 5.243 seconds
[2024-12-20T09:35:14.860+0000] {processor.py:161} INFO - Started process (PID=6464) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:35:14.861+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:35:14.863+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:14.862+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:35:15.329+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:15.328+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:35:15.544+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:35:15.545+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:15.545+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:35:15.840+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:35:15.841+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:15.841+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:35:15.875+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:35:15.876+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:15.875+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:35:16.428+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:35:16.429+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:16.429+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:35:19.727+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:35:19.774+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:19.773+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:35:19.821+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:19.821+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:35:19.854+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.999 seconds
[2024-12-20T09:35:50.016+0000] {processor.py:161} INFO - Started process (PID=6545) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:35:50.018+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:35:50.020+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:50.020+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:35:50.501+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:50.500+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:35:50.714+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:35:50.714+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:50.714+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:35:51.028+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:35:51.029+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:51.029+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:35:51.073+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:35:51.074+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:51.074+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:35:51.859+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:35:51.861+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:51.860+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:35:54.821+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:35:54.865+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:54.864+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:35:54.911+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:35:54.910+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:35:54.939+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.931 seconds
[2024-12-20T09:36:25.426+0000] {processor.py:161} INFO - Started process (PID=6628) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:36:25.428+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:36:25.430+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:36:25.429+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:36:25.891+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:36:25.891+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:36:26.163+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:36:26.164+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:36:26.164+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:36:26.455+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:36:26.456+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:36:26.455+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:36:26.493+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:36:26.494+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:36:26.494+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:36:27.061+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:36:27.062+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:36:27.061+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:36:30.006+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:36:30.051+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:36:30.050+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:36:30.109+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:36:30.109+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:36:30.148+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.727 seconds
[2024-12-20T09:37:00.392+0000] {processor.py:161} INFO - Started process (PID=6718) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:37:00.394+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:37:00.396+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:37:00.396+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:37:00.696+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:37:00.696+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:37:01.017+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:37:01.019+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:37:01.018+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:37:01.396+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:37:01.397+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:37:01.397+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:37:01.435+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:37:01.435+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:37:01.435+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:37:02.147+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:37:02.148+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:37:02.148+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:37:04.888+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:37:04.912+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:37:04.911+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:37:04.937+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:37:04.937+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:37:04.968+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.585 seconds
[2024-12-20T09:43:51.200+0000] {processor.py:161} INFO - Started process (PID=6796) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:43:51.202+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:43:51.203+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:43:51.202+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:43:51.641+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:43:51.640+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:43:51.964+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:43:51.964+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:43:51.964+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:43:52.307+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:43:52.308+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:43:52.308+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:43:52.351+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:43:52.352+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:43:52.351+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:43:53.180+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:43:53.181+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:43:53.181+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:43:57.164+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:43:57.205+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:43:57.204+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:43:57.249+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:43:57.248+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:43:57.277+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 6.083 seconds
[2024-12-20T09:44:27.513+0000] {processor.py:161} INFO - Started process (PID=6900) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:44:27.515+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:44:27.517+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:44:27.516+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:44:27.800+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:44:27.800+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:44:28.046+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:44:28.047+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:44:28.047+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:44:28.337+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:44:28.338+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:44:28.338+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:44:28.390+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:44:28.391+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:44:28.391+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:44:29.237+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:44:29.238+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:44:29.238+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:44:31.971+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:44:31.997+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:44:31.996+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:44:32.036+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:44:32.036+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:44:32.062+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.553 seconds
[2024-12-20T09:45:02.398+0000] {processor.py:161} INFO - Started process (PID=6973) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:45:02.399+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:45:02.400+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:02.400+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:45:02.627+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:02.626+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:45:06.765+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:45:06.766+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:06.766+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:45:07.009+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:45:07.010+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:07.010+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:45:07.044+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:45:07.045+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:07.044+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:45:07.954+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:45:07.955+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:07.955+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:45:10.833+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:45:10.857+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:10.856+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:45:10.886+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:10.886+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:45:10.912+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 8.519 seconds
[2024-12-20T09:45:41.789+0000] {processor.py:161} INFO - Started process (PID=7052) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:45:41.790+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:45:41.792+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:41.791+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:45:42.007+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:42.007+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:45:42.234+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:45:42.235+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:42.235+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:45:42.417+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:45:42.419+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:42.418+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:45:42.448+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:45:42.449+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:42.449+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:45:43.183+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:45:43.184+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:43.184+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:45:45.523+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:45:45.545+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:45.545+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:45:45.567+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:45:45.567+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:45:45.591+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.806 seconds
[2024-12-20T09:46:15.965+0000] {processor.py:161} INFO - Started process (PID=7131) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:46:15.966+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:46:15.968+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:15.967+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:46:16.176+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:16.176+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:46:16.406+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:46:16.407+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:16.407+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:46:16.650+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:46:16.650+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:16.650+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:46:16.683+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:46:16.684+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:16.684+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:46:17.161+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:46:17.162+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:17.161+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:46:19.298+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:46:19.322+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:19.321+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:46:19.341+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:19.341+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:46:19.361+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.401 seconds
[2024-12-20T09:46:49.514+0000] {processor.py:161} INFO - Started process (PID=7205) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:46:49.515+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:46:49.516+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:49.516+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:46:49.676+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:49.676+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:46:49.856+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:46:49.858+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:49.858+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:46:50.037+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:46:50.037+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:50.037+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:46:50.086+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:46:50.087+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:50.087+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:46:50.557+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:46:50.558+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:50.558+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:46:52.473+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:46:52.498+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:52.498+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:46:52.524+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:46:52.524+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:46:52.547+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.037 seconds
[2024-12-20T09:47:22.600+0000] {processor.py:161} INFO - Started process (PID=7293) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:47:22.601+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:47:22.602+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:22.602+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:47:22.790+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:22.789+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:47:22.984+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:47:22.985+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:22.985+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:47:23.171+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:47:23.172+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:23.172+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:47:23.203+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:47:23.204+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:23.204+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:47:23.740+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:47:23.741+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:23.741+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:47:25.994+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:47:26.015+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:26.014+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:47:26.039+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:26.039+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:47:26.064+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.468 seconds
[2024-12-20T09:47:56.930+0000] {processor.py:161} INFO - Started process (PID=7377) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:47:56.932+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:47:56.933+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:56.933+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:47:57.114+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:57.114+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:47:58.985+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:47:58.986+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:58.986+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:47:59.256+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:47:59.257+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:59.256+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:47:59.285+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:47:59.285+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:59.285+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:47:59.797+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:47:59.798+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:47:59.798+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:48:01.809+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:48:01.831+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:48:01.830+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:48:01.853+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:48:01.853+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:48:01.874+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 4.949 seconds
[2024-12-20T09:48:32.742+0000] {processor.py:161} INFO - Started process (PID=7481) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:48:32.744+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:48:32.745+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:48:32.744+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:48:32.908+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:48:32.908+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:48:33.088+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:48:33.089+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:48:33.089+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:48:33.261+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:48:33.262+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:48:33.262+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:48:33.303+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:48:33.304+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:48:33.304+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:48:33.754+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:48:33.755+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:48:33.755+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:48:35.712+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:48:35.733+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:48:35.732+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:48:35.754+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:48:35.754+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:48:35.776+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.038 seconds
[2024-12-20T09:49:06.545+0000] {processor.py:161} INFO - Started process (PID=7561) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:49:06.547+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:49:06.549+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:06.548+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:49:06.808+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:06.808+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:49:08.754+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:49:08.755+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:08.754+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:49:08.999+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:49:09.000+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:08.999+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:49:09.034+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:49:09.035+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:09.035+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:49:09.691+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:49:09.692+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:09.692+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:49:12.106+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:49:12.130+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:12.130+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:49:12.151+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:12.151+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:49:12.174+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 5.635 seconds
[2024-12-20T09:49:43.131+0000] {processor.py:161} INFO - Started process (PID=7633) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:49:43.132+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:49:43.133+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:43.133+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:49:43.303+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:43.303+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:49:43.499+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:49:43.500+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:43.499+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:49:43.669+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:49:43.670+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:43.670+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:49:43.708+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:49:43.709+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:43.709+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:49:44.155+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:49:44.155+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:44.155+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:49:46.244+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:49:46.280+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:46.279+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:49:46.315+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:49:46.315+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:49:46.341+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.214 seconds
[2024-12-20T09:50:16.772+0000] {processor.py:161} INFO - Started process (PID=7712) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:50:16.774+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:50:16.775+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:16.774+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:50:16.940+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:16.940+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:50:19.457+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:50:19.459+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:19.459+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:50:19.632+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:50:19.633+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:19.633+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:50:19.663+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:50:19.664+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:19.663+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:50:20.098+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:50:20.099+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:20.099+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:50:21.898+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:50:21.918+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:21.918+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:50:21.938+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:21.938+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:50:21.960+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 5.193 seconds
[2024-12-20T09:50:52.120+0000] {processor.py:161} INFO - Started process (PID=7791) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:50:52.121+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:50:52.122+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:52.122+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:50:52.305+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:52.305+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:50:52.526+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:50:52.527+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:52.526+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:50:52.710+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:50:52.711+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:52.711+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:50:52.735+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:50:52.736+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:52.736+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:50:53.226+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:50:53.227+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:53.227+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:50:55.093+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:50:55.115+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:55.115+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:50:55.136+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:50:55.135+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:50:55.164+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.048 seconds
[2024-12-20T09:51:25.655+0000] {processor.py:161} INFO - Started process (PID=7866) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:51:25.656+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:51:25.657+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:51:25.657+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:51:25.871+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:51:25.871+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:51:26.056+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:51:26.058+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:51:26.057+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:51:26.262+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:51:26.262+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:51:26.262+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:51:26.282+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:51:26.283+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:51:26.283+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:51:26.861+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:51:26.862+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:51:26.862+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:51:28.845+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:51:28.867+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:51:28.867+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:51:28.891+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:51:28.891+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:51:28.911+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.261 seconds
[2024-12-20T09:51:59.909+0000] {processor.py:161} INFO - Started process (PID=7965) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:51:59.910+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:51:59.912+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:51:59.911+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:52:00.111+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:00.110+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:52:00.298+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:52:00.299+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:00.299+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:52:00.471+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:52:00.472+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:00.472+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:52:00.502+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:52:00.503+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:00.503+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:52:00.940+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:52:00.941+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:00.941+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:52:02.876+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:52:02.897+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:02.896+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:52:02.923+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:02.923+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:52:02.944+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.038 seconds
[2024-12-20T09:52:33.101+0000] {processor.py:161} INFO - Started process (PID=8045) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:52:33.102+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:52:33.104+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:33.103+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:52:33.302+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:33.302+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:52:33.481+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:52:33.482+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:33.482+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:52:33.676+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:52:33.677+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:33.676+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:52:33.699+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:52:33.700+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:33.699+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:52:34.277+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:52:34.278+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:34.278+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:52:36.698+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:52:36.732+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:36.731+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:52:36.770+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:52:36.769+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:52:36.805+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.708 seconds
[2024-12-20T09:53:07.198+0000] {processor.py:161} INFO - Started process (PID=8139) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:53:07.199+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:53:07.200+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:07.200+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:53:07.371+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:07.371+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:53:11.259+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:53:11.260+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:11.259+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:53:11.446+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:53:11.447+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:11.447+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:53:11.475+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:53:11.476+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:11.476+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:53:12.009+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:53:12.010+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:12.010+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:53:14.341+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:53:14.364+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:14.364+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:53:14.387+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:14.387+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:53:14.415+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 7.223 seconds
[2024-12-20T09:53:44.481+0000] {processor.py:161} INFO - Started process (PID=8221) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:53:44.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:53:44.483+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:44.483+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:53:44.650+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:44.650+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:53:44.826+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:53:44.827+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:44.827+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:53:45.003+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:53:45.004+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:45.004+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:53:45.035+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:53:45.036+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:45.036+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:53:45.485+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:53:45.486+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:45.485+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:53:47.608+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:53:47.630+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:47.629+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:53:47.650+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:53:47.650+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:53:47.678+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.203 seconds
[2024-12-20T09:54:18.111+0000] {processor.py:161} INFO - Started process (PID=8300) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:54:18.112+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:54:18.113+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:54:18.112+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:54:18.287+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:54:18.287+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:54:27.753+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:54:27.754+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:54:27.754+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:54:27.947+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:54:27.947+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:54:27.947+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:54:27.974+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:54:27.975+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:54:27.975+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:54:28.477+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:54:28.478+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:54:28.478+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:54:30.685+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:54:30.709+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:54:30.709+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:54:30.739+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:54:30.739+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:54:30.770+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 12.662 seconds
[2024-12-20T09:55:01.078+0000] {processor.py:161} INFO - Started process (PID=8398) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:55:01.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:55:01.080+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:01.079+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:55:01.269+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:01.269+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:55:01.464+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:55:01.465+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:01.465+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:55:01.654+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:55:01.655+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:01.655+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:55:01.690+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:55:01.691+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:01.691+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:55:02.173+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:55:02.174+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:02.174+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:55:04.008+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:55:04.031+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:04.030+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:55:04.051+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:04.051+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:55:04.080+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.006 seconds
[2024-12-20T09:55:34.372+0000] {processor.py:161} INFO - Started process (PID=8475) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:55:34.373+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:55:34.374+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:34.374+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:55:34.537+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:34.537+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:55:40.157+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:55:40.158+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:40.157+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:55:40.336+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:55:40.337+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:40.337+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:55:40.365+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:55:40.366+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:40.366+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:55:40.814+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:55:40.815+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:40.815+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:55:42.619+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:55:42.641+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:42.640+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:55:42.662+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:55:42.662+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:55:42.685+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 8.317 seconds
[2024-12-20T09:56:12.941+0000] {processor.py:161} INFO - Started process (PID=8579) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:56:12.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:56:12.944+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:12.943+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:56:13.113+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:13.113+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:56:13.325+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:56:13.326+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:13.325+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:56:13.494+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:56:13.495+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:13.494+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:56:13.528+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:56:13.529+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:13.529+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:56:14.027+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:56:14.028+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:14.028+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:56:16.407+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:56:16.428+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:16.428+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:56:16.449+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:16.449+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:56:16.477+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.541 seconds
[2024-12-20T09:56:46.660+0000] {processor.py:161} INFO - Started process (PID=8659) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:56:46.662+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:56:46.663+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:46.663+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:56:46.838+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:46.837+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:56:47.043+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:56:47.044+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:47.044+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:56:47.283+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:56:47.284+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:47.284+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:56:47.313+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:56:47.314+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:47.314+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:56:47.789+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:56:47.790+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:47.790+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:56:49.914+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:56:49.936+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:49.936+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:56:49.958+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:56:49.958+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:56:49.979+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.323 seconds
[2024-12-20T09:57:20.396+0000] {processor.py:161} INFO - Started process (PID=8738) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:57:20.397+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:57:20.399+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:20.398+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:57:20.579+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:20.578+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:57:22.455+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:57:22.456+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:22.456+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:57:22.642+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:57:22.644+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:22.643+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:57:22.688+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:57:22.689+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:22.689+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:57:23.189+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:57:23.190+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:23.190+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:57:25.390+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:57:25.414+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:25.413+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:57:25.436+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:25.436+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:57:25.464+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 5.071 seconds
[2024-12-20T09:57:56.029+0000] {processor.py:161} INFO - Started process (PID=8810) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:57:56.030+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:57:56.032+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:56.031+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:57:56.216+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:56.216+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:57:56.425+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:57:56.425+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:56.425+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:57:56.620+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:57:56.621+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:56.621+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:57:56.650+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:57:56.651+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:56.651+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:57:57.259+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:57:57.260+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:57.259+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:57:59.337+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:57:59.358+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:59.358+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:57:59.380+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:57:59.380+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:57:59.403+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.379 seconds
[2024-12-20T09:58:29.973+0000] {processor.py:161} INFO - Started process (PID=8889) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:58:29.974+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:58:29.976+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:58:29.975+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:58:30.194+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:58:30.194+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:58:38.760+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:58:38.761+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:58:38.761+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:58:38.954+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:58:38.955+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:58:38.955+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:58:39.001+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:58:39.002+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:58:39.002+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:58:39.605+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:58:39.605+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:58:39.605+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:58:41.943+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:58:41.966+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:58:41.965+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:58:41.987+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:58:41.987+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:58:42.009+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 12.041 seconds
[2024-12-20T09:59:12.090+0000] {processor.py:161} INFO - Started process (PID=9009) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:59:12.092+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:59:12.093+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:12.092+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:59:12.273+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:12.273+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:59:12.495+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:59:12.496+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:12.496+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:59:12.704+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:59:12.705+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:12.705+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:59:12.732+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:59:12.734+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:12.734+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:59:13.238+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:59:13.239+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:13.238+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:59:15.017+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:59:15.045+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:15.045+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:59:15.067+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:15.067+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:59:15.087+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 3.001 seconds
[2024-12-20T09:59:45.866+0000] {processor.py:161} INFO - Started process (PID=9093) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:59:45.867+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T09:59:45.869+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:45.868+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:59:46.124+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:46.123+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T09:59:51.953+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T09:59:51.953+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:51.953+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T09:59:52.127+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:59:52.128+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:52.128+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:59:52.167+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:59:52.168+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:52.168+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:59:52.709+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T09:59:52.710+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:52.710+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T09:59:54.651+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T09:59:54.673+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:54.673+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T09:59:54.695+0000] {logging_mixin.py:188} INFO - [2024-12-20T09:59:54.694+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T09:59:54.723+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 8.864 seconds
[2024-12-20T10:00:25.077+0000] {processor.py:161} INFO - Started process (PID=9175) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:00:25.078+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:00:25.080+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:00:25.079+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:00:25.313+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:00:25.312+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:00:25.507+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:00:25.508+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:00:25.507+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:00:25.710+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:00:25.711+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:00:25.711+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:00:25.783+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:00:25.784+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:00:25.784+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:00:25.802+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:00:25.786+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:00:25.803+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:00:25.831+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.758 seconds
[2024-12-20T10:00:55.925+0000] {processor.py:161} INFO - Started process (PID=9240) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:00:55.927+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:00:55.929+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:00:55.928+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:00:56.175+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:00:56.175+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:00:56.378+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:00:56.379+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:00:56.379+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:00:56.555+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:00:56.556+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:00:56.556+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:00:56.592+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:00:56.593+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:00:56.592+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:00:56.609+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:00:56.594+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:00:56.610+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:00:56.632+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.712 seconds
[2024-12-20T10:01:27.074+0000] {processor.py:161} INFO - Started process (PID=9312) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:01:27.075+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:01:27.076+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:01:27.076+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:01:27.315+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:01:27.315+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:01:27.523+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:01:27.524+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:01:27.523+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:01:27.756+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:01:27.757+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:01:27.756+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:01:27.778+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:01:27.779+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:01:27.779+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:01:27.793+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:01:27.781+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:01:27.794+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:01:27.822+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.752 seconds
[2024-12-20T10:01:58.337+0000] {processor.py:161} INFO - Started process (PID=9377) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:01:58.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:01:58.340+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:01:58.340+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:01:58.535+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:01:58.534+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:01:58.738+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:01:58.739+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:01:58.739+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:01:58.939+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:01:58.940+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:01:58.940+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:01:58.971+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:01:58.972+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:01:58.972+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:01:58.988+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:01:58.975+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:01:58.989+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:01:59.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.680 seconds
[2024-12-20T10:02:29.193+0000] {processor.py:161} INFO - Started process (PID=9443) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:02:29.195+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:02:29.197+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:02:29.196+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:02:29.384+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:02:29.384+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:02:29.751+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:02:29.752+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:02:29.752+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:02:29.978+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:02:29.979+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:02:29.979+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:02:30.000+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:02:30.000+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:02:30.000+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:02:30.011+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:02:30.002+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:02:30.012+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:02:30.034+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.847 seconds
[2024-12-20T10:03:00.290+0000] {processor.py:161} INFO - Started process (PID=9508) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:03:00.292+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:03:00.294+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:03:00.293+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:03:00.517+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:03:00.516+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:03:00.730+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:03:00.731+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:03:00.731+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:03:00.989+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:03:00.990+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:03:00.990+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:03:01.013+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:03:01.014+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:03:01.013+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:03:01.025+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:03:01.015+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:03:01.026+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:03:01.236+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.951 seconds
[2024-12-20T10:03:31.453+0000] {processor.py:161} INFO - Started process (PID=9580) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:03:31.454+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:03:31.455+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:03:31.455+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:03:31.627+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:03:31.627+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:03:31.817+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:03:31.818+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:03:31.818+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:03:31.996+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:03:31.997+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:03:31.997+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:03:32.025+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:03:32.026+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:03:32.026+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:03:32.042+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:03:32.028+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:03:32.043+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:03:32.072+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.623 seconds
[2024-12-20T10:04:02.655+0000] {processor.py:161} INFO - Started process (PID=9645) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:04:02.656+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:04:02.657+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:04:02.657+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:04:02.830+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:04:02.830+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:04:03.039+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:04:03.040+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:04:03.040+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:04:03.208+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:04:03.209+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:04:03.209+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:04:03.238+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:04:03.239+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:04:03.238+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:04:03.255+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:04:03.240+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:04:03.256+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:04:03.279+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.627 seconds
[2024-12-20T10:04:33.834+0000] {processor.py:161} INFO - Started process (PID=9717) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:04:33.835+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:04:33.836+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:04:33.836+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:04:34.008+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:04:34.008+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:04:34.188+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:04:34.189+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:04:34.188+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:04:34.385+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:04:34.385+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:04:34.385+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:04:34.409+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:04:34.410+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:04:34.409+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:04:34.425+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:04:34.412+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:04:34.426+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:04:34.450+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.621 seconds
[2024-12-20T10:05:05.263+0000] {processor.py:161} INFO - Started process (PID=9782) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:05:05.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:05:05.265+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:05:05.265+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:05:05.447+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:05:05.447+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:05:05.648+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:05:05.649+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:05:05.649+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:05:05.818+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:05:05.819+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:05:05.819+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:05:06.006+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:05:06.007+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:05:06.007+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:05:06.025+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:05:06.009+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:05:06.026+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:05:06.048+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.789 seconds
[2024-12-20T10:05:36.544+0000] {processor.py:161} INFO - Started process (PID=9848) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:05:36.546+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:05:36.547+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:05:36.547+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:05:36.723+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:05:36.723+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:05:36.911+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:05:36.913+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:05:36.912+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:05:37.077+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:05:37.078+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:05:37.078+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:05:37.112+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:05:37.113+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:05:37.113+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:05:37.127+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:05:37.115+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:05:37.128+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:05:37.157+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.618 seconds
[2024-12-20T10:06:07.273+0000] {processor.py:161} INFO - Started process (PID=9921) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:06:07.274+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:06:07.275+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:06:07.275+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:06:07.510+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:06:07.510+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:06:07.715+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:06:07.717+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:06:07.716+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:06:07.923+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:06:07.924+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:06:07.923+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:06:07.962+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:06:07.962+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:06:07.962+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:06:07.978+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:06:07.964+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:06:07.979+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:06:08.006+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.738 seconds
[2024-12-20T10:06:38.680+0000] {processor.py:161} INFO - Started process (PID=9987) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:06:38.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:06:38.683+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:06:38.683+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:06:38.877+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:06:38.877+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:06:39.457+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:06:39.458+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:06:39.458+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:06:39.774+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:06:39.775+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:06:39.774+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:06:39.802+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:06:39.803+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:06:39.802+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:06:39.817+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:06:39.804+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:06:39.818+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:06:39.845+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 1.169 seconds
[2024-12-20T10:07:10.419+0000] {processor.py:161} INFO - Started process (PID=10062) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:07:10.420+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:07:10.421+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:07:10.421+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:07:10.635+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:07:10.634+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:07:10.830+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:07:10.831+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:07:10.831+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:07:11.026+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:07:11.027+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:07:11.026+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:07:11.218+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:07:11.219+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:07:11.219+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:07:11.235+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:07:11.222+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:07:11.236+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:07:11.254+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.840 seconds
[2024-12-20T10:07:41.581+0000] {processor.py:161} INFO - Started process (PID=10128) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:07:41.582+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:07:41.583+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:07:41.583+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:07:41.808+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:07:41.808+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:07:41.993+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:07:41.994+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:07:41.994+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:07:42.216+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:07:42.217+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:07:42.217+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:07:42.245+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:07:42.246+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:07:42.245+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:07:42.258+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:07:42.247+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:07:42.259+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:07:42.281+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.705 seconds
[2024-12-20T10:08:12.418+0000] {processor.py:161} INFO - Started process (PID=10201) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:08:12.420+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:08:12.421+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:08:12.421+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:08:12.613+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:08:12.613+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:08:12.808+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:08:12.809+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:08:12.808+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:08:13.216+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:08:13.217+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:08:13.217+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:08:13.247+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:08:13.247+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:08:13.247+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:08:13.260+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:08:13.249+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:08:13.261+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:08:13.283+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.869 seconds
[2024-12-20T10:08:44.214+0000] {processor.py:161} INFO - Started process (PID=10272) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:08:44.215+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:08:44.216+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:08:44.216+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:08:44.429+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:08:44.428+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:08:44.627+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:08:44.627+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:08:44.627+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:08:44.996+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:08:44.996+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:08:44.996+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:08:45.023+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:08:45.024+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:08:45.024+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:08:45.037+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:08:45.025+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:08:45.038+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:08:45.057+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.847 seconds
[2024-12-20T10:09:15.627+0000] {processor.py:161} INFO - Started process (PID=10337) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:09:15.629+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:09:15.631+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:09:15.630+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:09:15.851+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:09:15.851+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:09:17.647+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:09:17.648+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:09:17.648+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:09:17.897+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:09:17.898+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:09:17.898+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:09:18.067+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:09:18.068+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:09:18.068+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:09:18.081+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:09:18.070+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:09:18.082+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:09:18.100+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 2.479 seconds
[2024-12-20T10:09:48.972+0000] {processor.py:161} INFO - Started process (PID=10420) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:09:48.973+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:09:48.974+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:09:48.974+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:09:49.160+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:09:49.160+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:09:49.364+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:09:49.365+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:09:49.365+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:09:49.584+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:09:49.585+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:09:49.585+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:09:49.777+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:09:49.778+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:09:49.778+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:09:49.797+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:09:49.780+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:09:49.798+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:09:49.824+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.856 seconds
[2024-12-20T10:10:20.028+0000] {processor.py:161} INFO - Started process (PID=10485) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:10:20.029+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:10:20.030+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:10:20.030+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:10:20.253+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:10:20.253+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:10:20.490+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:10:20.491+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:10:20.491+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:10:20.844+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:10:20.845+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:10:20.845+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:10:20.888+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:10:20.888+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:10:20.888+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:10:20.905+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:10:20.890+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:10:20.906+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:10:20.928+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 0.904 seconds
[2024-12-20T10:10:51.235+0000] {processor.py:161} INFO - Started process (PID=10557) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:10:51.236+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:10:51.237+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:10:51.237+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:10:51.418+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:10:51.418+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:10:59.548+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:10:59.549+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:10:59.548+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:10:59.873+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:10:59.874+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:10:59.874+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:10:59.910+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:10:59.911+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:10:59.911+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:10:59.927+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:10:59.913+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/training_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/training_pipeline.py", line 34, in <module>
    from data.load_data_s3_dagshub_airflow import upload_all
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 25, in <module>
    upload_all()
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 21, in upload_all
    upload_file(path, "tsunamis.csv")
  File "/opt/airflow/src/data/load_data_s3_dagshub_airflow.py", line 14, in upload_file
    s3.upload_file(
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/futures.py", line 266, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/tasks.py", line 269, in _main
    self._submit(transfer_future=transfer_future, **kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 591, in _submit
    upload_input_manager.provide_transfer_size(transfer_future)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/upload.py", line 244, in provide_transfer_size
    self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)
  File "/home/airflow/.local/lib/python3.11/site-packages/s3transfer/utils.py", line 251, in get_file_size
    return os.path.getsize(filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 50, in getsize
FileNotFoundError: [Errno 2] No such file or directory: 'opt/airflow/data/raw/tsunamis.csv'
[2024-12-20T10:10:59.928+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:10:59.954+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 8.723 seconds
[2024-12-20T10:11:30.368+0000] {processor.py:161} INFO - Started process (PID=10652) to work on /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:11:30.370+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/training_pipeline.py for tasks to queue
[2024-12-20T10:11:30.371+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:11:30.371+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:11:30.553+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:11:30.553+0000] {tokens.py:86} WARNING - The added token already exists in the token cache, skipping
[2024-12-20T10:11:30.748+0000] {logging_mixin.py:188} INFO - Accessing as sarahlunette
[2024-12-20T10:11:30.748+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:11:30.748+0000] {helpers.py:107} INFO - Accessing as sarahlunette
[2024-12-20T10:11:31.123+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:11:31.124+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:11:31.124+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:11:31.173+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:11:31.173+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:11:31.173+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:11:35.175+0000] {logging_mixin.py:188} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the 
bucket
[2024-12-20T10:11:35.176+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:11:35.176+0000] {helpers.py:107} INFO - Client created. Use the name of the repo (Data_Atelier) as the name of the bucket
[2024-12-20T10:11:37.665+0000] {processor.py:840} INFO - DAG(s) 'training_pipeline' retrieved from /opt/airflow/dags/training_pipeline.py
[2024-12-20T10:11:37.819+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:11:37.818+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2024-12-20T10:11:38.019+0000] {logging_mixin.py:188} INFO - [2024-12-20T10:11:38.018+0000] {dag.py:3823} INFO - Setting next_dagrun for training_pipeline to 2024-12-20 00:00:00+00:00, run_after=2024-12-21 00:00:00+00:00
[2024-12-20T10:11:38.051+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/training_pipeline.py took 7.691 seconds
